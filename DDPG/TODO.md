Print out the rewards every timestep to make sure they are being represented correctly. The observed q values should be near the average observed value from the environment, so make sure this lines up (are we getting positive or negative reward)

UPDATE: Q avg and value function (critic) seem feasible. Seems to be some error in taking the action. The action distribution over the episode just has one small blip
